# Team BlueToad at SemEval 2025 Task 3 Mu-SHROOM
This repository contains the code which let us tune Large Language Models with which we participated in the 2025 SemEval Shared Task 3.
The aim of the task was to mark hallucinations in text generated by language models.
We used ChatGPT4o-mini to annotate the English training data and used this with the validation data to fine-tune several LLMs.
The repository contains the code to fine-tune the pre-trained LLMs and to evaluate them on given test data.

The tuner class can be run with the following command:

```
python3 MushroomQaTuner.py \
 --train_file=train_file_path\
 --val_file=val_file_path \
 --test_file=test_file_path \
 --model_name=hugging_face_model_name \
 --model_output_dir=output_directory_name \
 --predictions_filename=prediction_save_destination \
 --max_length=384 \
 --stride=128 \
 --n_best=20 \
 --max_answer_length=30 \
 --learning_rate=2e-05 \
 --batch_size=16 \
 --epochs=3 \
 --weight_decay=0.1
```

A model can be loaded an evaluated with the compute_metrics.py script:

```
python3 compute_metrics.py model_name evaluation_file_path predictions_output_file_path
```

Our best submitted model can be found here https://huggingface.co/MichielPronk/xlm-roberta-mushroom-qa
