import pandas as pd
import argparse
import requests
import json
import os
import re


def preprocess(string):
    string = string.replace("\n", "")

    return string

def get_input(df):

    """
    Takes one random sample from the the training set DataFrame,
    extracts the model input and output values from the sample,
    and returns them in a prompt format as a string
    """

    # Get a random sample from the data frame
    sample = df.sample(1)

    # Drop that column to ensure the model does not choose the same sample twice
    new_df = df.drop(sample.index)
    source = preprocess(sample["model_input"].values[0])
    answer = preprocess(sample["model_output_text"].values[0])

    # Prepare the input for the prompt
    source_text = "SOURCE TEXT:" + source
    answer_text = "ANSWER TEXT:" + answer
    sample = source_text + "\n" + answer_text

    return sample, answer, new_df
        

def create_prompt(input):

    """
    Receives the formatted model input and output as one string,
    appends it to the created prompt, and returns it as a 
    part of the GPT instruction formatted as a dictionary
    """

    with open("./gpt_prompts/prompt.txt") as f:
        data = f.read()

    prompt = data + "\n\n" + input
    prompt_json = {"role": "user", "content": prompt}

    return prompt_json


def prompt_gpt(prompt):

    """
    Receives a prompt as a string, prompts gpt-4-turbo and
    returns its response as a string
    """

    with open ("OPEN_AI_KEY.txt") as f:
        openai_api_key = f.read()

    if openai_api_key is None:
        raise ValueError("OpenAI API key is not set in environment variables.")

    url = "https://api.openai.com/v1/chat/completions"

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {openai_api_key}"
    }

    data = {
        "model": "gpt-4-turbo",
        "messages": [
            prompt
        ],
        "temperature": 0.2
    }

    response = requests.post(url, headers=headers, json=data)

    # Check if the request was successful
    if response.status_code == 200:
        print("Response from OpenAI:", response.json())
        print('\n')
        gpt_answer = response.json()['choices'][0]['message']['content']
    else:
        print("Error:", response.status_code, response.text)

    return gpt_answer


def process_gpt_output(gpt_answer):

    """
    Receives the GPT output as a string, extracts the hallucination
    spans and returns them as a list of substrings
    """

    # Get rid of possible GPT-generated text before the spans
    gpt_spans = gpt_answer.split(':', 1)[1].strip()

    # Split the spans in a list
    substrings = gpt_spans.split(", ")

    # Remove generated by GPT double quotes and separators in the output
    pattern = re.compile(r'["]|[,]')
    return [pattern.sub('', item) for item in substrings]


def find_spans(source, substrings):

    """
    Receives the source text as a string and a list of extracted from it substrings,
    provides the numerical start and end spans of each substring in the source,
    and returns them in a list
    """

    spans = []
    used_indices = set()  # To track already matched positions in the text

    for substring in substrings:
        # Use \b to ensure whole-word matching
        for match in re.finditer(rf'\b{re.escape(substring)}\b', source):
            start, end = match.span()
            # Check if this span overlaps with previously matched ones
            if all(start >= end_used or end <= start_used for start_used, end_used in used_indices):
                spans.append((start, end))
                used_indices.add((start, end))
                break  # Stop after the first match for this target word

    return spans


def merge_spans(spans):

    """
    Receives a list of spans as tuples, loops through it to
    merges the consequtive spans into "hard labels", then
    returns them as a list of tuples
    """

    spans = [r for r in spans if r is not None]

    spans.sort()

    merged = []
    for start, end in spans:
        if merged and start <= merged[-1][1] + 1:
            merged[-1] = (merged[-1][0], max(merged[-1][1], end))
        else:
            merged.append((start, end))

    return merged


def write_json(data, args):

    """
    Takes the data written as a dictionary and
    appends it in a .jsonl file
    """

    # Specify the filename for the JSON file (adjusted the name to not overwrite existend files)
    json_file = f'./data/train_synthetic/mushroom.{args.language}-train_LABELED_NEW.v1.jsonl'

    # Check if the JSON lines file exists
    if not os.path.exists(json_file):
        # If it doesn't exist, create the file
        with open(json_file, "w") as f:
            pass
        print(f"{json_file} created.")

    # Open the file in append mode and add a new row
    with open(json_file, "a") as file:
        file.write(json.dumps(data) + "\n")  # Write as JSON line
    print("New row added to the file.")


def create_arg_parser():

    """
    Create an argument parser for the MushroomQaTuner class.

    Returns:
        args (Namespace): The arguments passed in from the command line.
    """

    parser = argparse.ArgumentParser()
    parser.add_argument("-n", "--n_samples", default=500, type=int,
                        help="Define the number of entries in the unlebeld train set to be processed by ChatGPT")
    parser.add_argument("-l","--language", default="en", type=str, choices=["en", "es", "fr", "zh"],
                        help="Define the language of the unlabeled training set for processing")

    args = parser.parse_args()
    return args


def main():

    args = create_arg_parser()
    df = pd.read_json(path_or_buf=f"./data/train_unlabeled/mushroom.{args.language}-train_nolabel.v1.jsonl", lines=True)

    for i in range(args.n_samples):

        # Prepare the input from the training data and the prompt
        input_sample, answer_text, df = get_input(df)
        prompt = create_prompt(input_sample)

        # Prompt and process the GPT-generated output
        gpt_answer = prompt_gpt(prompt)
        substrings = process_gpt_output(gpt_answer)

        # Convert the received output substrings into hard labels
        spans = find_spans(answer_text, substrings)
        merged_spans = merge_spans(spans)

        # Prepare the necessary data from the input training set
        source = input_sample.split(":")
        input = source[1].split("\n")
        input = input[0]
        source = source[-1]

        data = {
            "model_input": input,
            "model_output_text": source,
            "hard_labels": merged_spans
        }

        write_json(data, args)


if __name__ == "__main__":
    main()
